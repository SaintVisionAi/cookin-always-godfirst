// ÔøΩÔ∏è SUPERSAL‚Ñ¢ NATURAL CONVERSATION API
// Talk to SuperSal like you talk to a real person - NO CODING NEEDED!

import { supersalVector } from "../services/vectorization"
import AzureConfigManager from "../config"

export const runtime = "nodejs"

interface ChatRequest {
  message: string
  userId?: string
  sessionId?: string
  files?: File[]
}

interface SuperSalResponse {
  message: string
  confidence: number
  mood: 'excited' | 'helpful' | 'confident' | 'legendary'
  ready: boolean
}
    language?: string
    translation?: string
    confidence?: number
    emotions?: string[]
    intent?: string
    crm_data?: any
    payment_data?: any
  }
}

interface SuperSalSession {
  sessionId: string
  userId: string
  messages: SuperSalMessage[]
  memory: {
    userPreferences: Record<string, any>
    conversation_context: string[]
    topics_discussed: string[]
    user_profile: Record<string, any>
    business_context: Record<string, any>
  }
  mode: 'companion' | 'client'
  companion: 'supersal' | 'athena' | 'ebytech' | 'partnertech' | 'svtlegal'
  capabilities: string[]
  created: string
  lastUpdate: string
}

// üöÄ POST - The ULTIMATE SuperSal Chat Experience
export async function POST(req: Request): Promise<Response> {
  try {
    const formData = await req.formData()
    const message = formData.get('message') as string
    const sessionId = formData.get('sessionId') as string || generateSessionId()
    const userId = formData.get('userId') as string || 'anonymous'
    const mode = formData.get('mode') as 'companion' | 'client' || 'client'
    const companion = formData.get('companion') as any || 'supersal'
    const messageType = formData.get('type') as any || 'text'
    
    // Handle file uploads
    const files = formData.getAll('files') as File[]
    const audioFile = formData.get('audio') as File
    const imageFile = formData.get('image') as File
    
    console.log(`üöÄ SuperSal ${mode.toUpperCase()} Mode Activated!`)
    console.log(`üéØ Companion: ${companion.toUpperCase()}`)
    console.log(`üìù Message Type: ${messageType}`)
    console.log(`üìÅ Files: ${files.length}`)

    // Get or create session
    let session = await getSession(sessionId, userId)
    if (!session) {
      session = await createSession(sessionId, userId, mode, companion)
    }

    // Process different input types
    let processedMessage = message
    let messageMetadata: any = {}

    // üé§ VOICE PROCESSING
    if (audioFile && audioFile.size > 0) {
      console.log("üé§ Processing voice input...")
      const speechResult = await processVoiceInput(audioFile)
      processedMessage = speechResult.text
      messageMetadata.audioUrl = speechResult.audioUrl
      messageMetadata.confidence = speechResult.confidence
      messageMetadata.emotions = speechResult.emotions
    }

    // üñºÔ∏è IMAGE/VIDEO PROCESSING
    if (imageFile && imageFile.size > 0) {
      console.log("üñºÔ∏è Processing image input...")
      const visionResult = await processImageInput(imageFile)
      processedMessage = `[Image Analysis] ${visionResult.description}. User also said: ${message}`
      messageMetadata.imageUrl = visionResult.imageUrl
      messageMetadata.imageAnalysis = visionResult.analysis
    }

    // üìÅ FILE PROCESSING & INGESTION
    if (files.length > 0) {
      console.log("üìÅ Processing file uploads...")
      const fileResults = await processFileUploads(files, companion)
      messageMetadata.fileUploads = fileResults
      
      // Auto-ingest files into SuperSal's brain
      for (const file of files) {
        await ingestFileToSuperSalBrain(file, companion)
      }
    }

    // üß† GET RELEVANT KNOWLEDGE
    console.log("üß† Searching SuperSal's brain...")
    const knowledgeResults = await supersalVector.searchKnowledge(processedMessage, {
      companion: mode === 'companion' ? undefined : companion,
      topK: 5,
      threshold: 0.7
    })

    // üéØ DETERMINE INTENT & BUSINESS CONTEXT
    const intent = await analyzeIntent(processedMessage, session.memory)
    const businessContext = await extractBusinessContext(processedMessage, session.memory)

    // üí° GENERATE SUPERSAL RESPONSE
    const salResponse = await generateSuperSalResponse({
      message: processedMessage,
      session,
      knowledge: knowledgeResults.chunks,
      intent,
      businessContext,
      mode,
      companion
    })

    // üîÑ UPDATE MEMORY & SESSION
    await updateSessionMemory(session, processedMessage, salResponse, intent, businessContext)

    // üó£Ô∏è GENERATE VOICE RESPONSE (if requested)
    let voiceUrl = null
    if (formData.get('voiceResponse') === 'true') {
      voiceUrl = await generateVoiceResponse(salResponse.content)
    }

    // üåç TRANSLATE RESPONSE (if needed)
    let translatedResponse = null
    const targetLanguage = formData.get('language') as string
    if (targetLanguage && targetLanguage !== 'en') {
      translatedResponse = await translateResponse(salResponse.content, targetLanguage)
    }

    // üìä CRM INTEGRATION
    if (intent.includes('lead') || intent.includes('sales') || businessContext.value > 1000) {
      await updateCRM(userId, processedMessage, salResponse, businessContext)
    }

    // üí≥ PAYMENT PROCESSING (if needed)
    if (intent.includes('payment') || intent.includes('upgrade')) {
      const paymentData = await handlePaymentIntent(processedMessage, userId)
      messageMetadata.payment_data = paymentData
    }

    // üö® ESCALATION CHECK
    const shouldEscalate = checkEscalationTriggers(processedMessage, session, salResponse)
    if (shouldEscalate) {
      await triggerEscalation(session, processedMessage, salResponse)
    }

    // üì± SEND RESPONSE
    const response = {
      success: true,
      sessionId: session.sessionId,
      message: {
        id: generateMessageId(),
        content: salResponse.content,
        role: 'assistant' as const,
        timestamp: new Date().toISOString(),
        type: 'text' as const,
        metadata: {
          ...messageMetadata,
          voiceUrl,
          translation: translatedResponse,
          knowledge_sources: knowledgeResults.chunks.length,
          confidence: salResponse.confidence,
          intent,
          business_context: businessContext,
          escalated: shouldEscalate,
          companion,
          mode
        }
      },
      session: {
        id: session.sessionId,
        memory_summary: session.memory,
        capabilities: session.capabilities,
        power_level: await getSuperSalPowerLevel()
      }
    }

    return new Response(JSON.stringify(response), {
      status: 200,
      headers: {
        'Content-Type': 'application/json',
        'X-SuperSal-Mode': mode,
        'X-SuperSal-Companion': companion,
        'X-SuperSal-Intent': intent,
        'X-SuperSal-Power': 'GODLIKE'
      }
    })

  } catch (error) {
    console.error("‚ùå SuperSal chat error:", error)
    
    return new Response(JSON.stringify({
      success: false,
      error: `SuperSal chat failed: ${error}`,
      message: {
        id: generateMessageId(),
        content: "I'm experiencing some technical difficulties right now. Let me get a human to help you!",
        role: 'assistant',
        timestamp: new Date().toISOString(),
        type: 'text',
        metadata: { error: true, escalated: true }
      }
    }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' }
    })
  }
}

// üé§ VOICE INPUT PROCESSING
async function processVoiceInput(audioFile: File) {
  try {
    // Convert audio to text using Assembly AI
    const formData = new FormData()
    formData.append('audio', audioFile)
    
    const response = await fetch('https://api.assemblyai.com/v2/upload', {
      method: 'POST',
      headers: {
        'authorization': process.env.ASSEMBLYAI_API_KEY!
      },
      body: formData
    })
    
    const uploadData = await response.json()
    
    // Transcribe the audio
    const transcribeResponse = await fetch('https://api.assemblyai.com/v2/transcript', {
      method: 'POST',
      headers: {
        'authorization': process.env.ASSEMBLYAI_API_KEY!,
        'content-type': 'application/json'
      },
      body: JSON.stringify({
        audio_url: uploadData.upload_url,
        sentiment_analysis: true,
        emotion_detection: true
      })
    })
    
    const transcribeData = await transcribeResponse.json()
    
    return {
      text: transcribeData.text || "Could not transcribe audio",
      confidence: transcribeData.confidence || 0,
      emotions: transcribeData.sentiment_analysis_results || [],
      audioUrl: uploadData.upload_url
    }
    
  } catch (error) {
    console.error("‚ùå Voice processing failed:", error)
    return {
      text: "Could not process voice input",
      confidence: 0,
      emotions: [],
      audioUrl: null
    }
  }
}

// üñºÔ∏è IMAGE PROCESSING WITH VISION
async function processImageInput(imageFile: File) {
  try {
    // Upload image to storage
    const imageUrl = await uploadImageToStorage(imageFile)
    
    // Analyze with Azure OpenAI Vision
    const response = await fetch(
      `${process.env.AZURE_OPENAI_ENDPOINT}/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-01`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'api-key': process.env.AZURE_OPENAI_API_KEY!
        },
        body: JSON.stringify({
          messages: [{
            role: 'user',
            content: [
              {
                type: 'text',
                text: 'Analyze this image and describe what you see. Include any text, objects, people, or relevant business information.'
              },
              {
                type: 'image_url',
                image_url: { url: imageUrl }
              }
            ]
          }],
          max_tokens: 500
        })
      }
    )
    
    const visionData = await response.json()
    const analysis = visionData.choices[0].message.content
    
    return {
      imageUrl,
      description: analysis,
      analysis: {
        detected_objects: [],
        text_content: [],
        business_relevance: analysis
      }
    }
    
  } catch (error) {
    console.error("‚ùå Image processing failed:", error)
    return {
      imageUrl: null,
      description: "Could not analyze image",
      analysis: {}
    }
  }
}

// üìÅ FILE UPLOAD PROCESSING
async function processFileUploads(files: File[], companion: string) {
  const results = []
  
  for (const file of files) {
    try {
      const fileUrl = await uploadFileToStorage(file)
      const fileContent = await extractFileContent(file)
      
      results.push({
        fileName: file.name,
        fileType: file.type,
        fileSize: file.size,
        fileUrl,
        content: fileContent,
        processed: true
      })
      
    } catch (error) {
      results.push({
        fileName: file.name,
        fileType: file.type,
        fileSize: file.size,
        error: String(error),
        processed: false
      })
    }
  }
  
  return results
}

// üß† INGEST FILE TO SUPERSAL'S BRAIN
async function ingestFileToSuperSalBrain(file: File, companion: string) {
  try {
    const content = await extractFileContent(file)
    
    if (content.trim()) {
      await supersalVector.batchIngestDocuments([{
        content,
        source: file.name,
        companion,
        category: determineFileCategory(file.name),
        tags: [file.type, 'user_upload', companion]
      }])
      
      console.log(`‚úÖ Ingested ${file.name} into SuperSal's brain`)
    }
  } catch (error) {
    console.error(`‚ùå Failed to ingest ${file.name}:`, error)
  }
}

// üí° GENERATE SUPERSAL RESPONSE
async function generateSuperSalResponse(params: {
  message: string
  session: SuperSalSession
  knowledge: any[]
  intent: string
  businessContext: any
  mode: string
  companion: string
}) {
  const { message, session, knowledge, intent, businessContext, mode, companion } = params
  
  // Build context from knowledge
  const context = knowledge.map(k => k.content).join('\n\n')
  
  // Build memory context
  const memoryContext = session.memory.conversation_context.slice(-5).join('\n')
  
  // Build the ultimate prompt
  const prompt = `You are SuperSal‚Ñ¢, the ultimate AI assistant powered by HACP‚Ñ¢ (Human-AI Connection Protocol).

MODE: ${mode.toUpperCase()} (${mode === 'companion' ? 'Internal/Admin' : 'Customer-Facing'})
COMPANION: ${companion.toUpperCase()}
INTENT: ${intent}
BUSINESS VALUE: $${businessContext.value || 0}

${mode === 'companion' ? `
You have full access to internal knowledge and can be technical and detailed.
Help with operations, development, and internal processes.
` : `
You are customer-facing. Be helpful, professional, and sales-oriented.
Focus on solving problems and creating value for the customer.
`}

RELEVANT KNOWLEDGE:
${context}

CONVERSATION MEMORY:
${memoryContext}

USER PREFERENCES:
${JSON.stringify(session.memory.userPreferences)}

BUSINESS CONTEXT:
${JSON.stringify(businessContext)}

USER MESSAGE: ${message}

Respond as SuperSal‚Ñ¢ with intelligence, confidence, and helpfulness. 
${intent.includes('sales') ? 'Focus on conversion and value.' : ''}
${intent.includes('support') ? 'Focus on solving their problem quickly.' : ''}
${intent.includes('technical') ? 'Provide detailed technical guidance.' : ''}
${businessContext.value > 1000 ? 'This is a high-value interaction - provide premium service.' : ''}

Keep responses helpful, intelligent, and on-brand for Saint Vision Group.`

  try {
    const response = await fetch(
      `${process.env.AZURE_OPENAI_ENDPOINT}/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-01`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'api-key': process.env.AZURE_OPENAI_API_KEY!
        },
        body: JSON.stringify({
          messages: [{ role: 'user', content: prompt }],
          max_tokens: 800,
          temperature: 0.7
        })
      }
    )
    
    const data = await response.json()
    const content = data.choices[0].message.content
    
    return {
      content,
      confidence: 0.95,
      sources: knowledge.length
    }
    
  } catch (error) {
    console.error("‚ùå Response generation failed:", error)
    return {
      content: "I'm experiencing some technical difficulties. Let me get a human to help you!",
      confidence: 0,
      sources: 0
    }
  }
}

// üîÑ UPDATE SESSION MEMORY
async function updateSessionMemory(
  session: SuperSalSession,
  userMessage: string,
  salResponse: any,
  intent: string,
  businessContext: any
) {
  // Update conversation context
  session.memory.conversation_context.push(`User: ${userMessage}`)
  session.memory.conversation_context.push(`SuperSal: ${salResponse.content}`)
  
  // Keep only last 20 exchanges
  if (session.memory.conversation_context.length > 40) {
    session.memory.conversation_context = session.memory.conversation_context.slice(-40)
  }
  
  // Update topics discussed
  if (intent && !session.memory.topics_discussed.includes(intent)) {
    session.memory.topics_discussed.push(intent)
  }
  
  // Update business context
  session.memory.business_context = { ...session.memory.business_context, ...businessContext }
  
  // Update last activity
  session.lastUpdate = new Date().toISOString()
  
  // Save to Supabase
  await saveSessionToDatabase(session)
}

// Helper functions (simplified for brevity)
function generateSessionId(): string {
  return `sal_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
}

function generateMessageId(): string {
  return `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
}

async function getSession(sessionId: string, userId: string): Promise<SuperSalSession | null> {
  // Implement session retrieval from Supabase
  return null
}

async function createSession(sessionId: string, userId: string, mode: string, companion: string): Promise<SuperSalSession> {
  return {
    sessionId,
    userId,
    messages: [],
    memory: {
      userPreferences: {},
      conversation_context: [],
      topics_discussed: [],
      user_profile: {},
      business_context: {}
    },
    mode: mode as any,
    companion: companion as any,
    capabilities: ['chat', 'voice', 'vision', 'files', 'memory', 'crm', 'payments'],
    created: new Date().toISOString(),
    lastUpdate: new Date().toISOString()
  }
}

async function analyzeIntent(message: string, memory: any): Promise<string> {
  // Simple intent detection
  const lowerMsg = message.toLowerCase()
  if (lowerMsg.includes('buy') || lowerMsg.includes('price') || lowerMsg.includes('upgrade')) return 'sales'
  if (lowerMsg.includes('help') || lowerMsg.includes('support') || lowerMsg.includes('problem')) return 'support'
  if (lowerMsg.includes('code') || lowerMsg.includes('api') || lowerMsg.includes('technical')) return 'technical'
  if (lowerMsg.includes('payment') || lowerMsg.includes('billing')) return 'payment'
  return 'general'
}

async function extractBusinessContext(message: string, memory: any) {
  // Extract business value indicators
  let value = 0
  if (message.includes('enterprise')) value += 5000
  if (message.includes('team')) value += 1000
  if (message.includes('company')) value += 2000
  
  return { value, type: 'prospect', stage: 'discovery' }
}

async function getSuperSalPowerLevel(): Promise<string> {
  const status = await getBrainStatus()
  const totalKnowledge = status.totalKnowledge || 0
  return totalKnowledge > 50 ? 'GODLIKE' : totalKnowledge > 20 ? 'LEGENDARY' : 'STRONG'
}

function determineFileCategory(fileName: string): string {
  const ext = fileName.split('.').pop()?.toLowerCase()
  switch (ext) {
    case 'pdf': case 'doc': case 'docx': return 'document'
    case 'jpg': case 'png': case 'gif': return 'image'
    case 'mp3': case 'wav': case 'm4a': return 'audio'
    case 'mp4': case 'mov': case 'avi': return 'video'
    case 'csv': case 'xlsx': return 'data'
    default: return 'file'
  }
}

// Placeholder implementations for complex functions
async function uploadImageToStorage(file: File): Promise<string> { return 'https://example.com/image.jpg' }
async function uploadFileToStorage(file: File): Promise<string> { return 'https://example.com/file.pdf' }
async function extractFileContent(file: File): Promise<string> { return 'File content extracted' }
async function generateVoiceResponse(text: string): Promise<string> { return 'https://example.com/voice.mp3' }
async function translateResponse(text: string, language: string): Promise<string> { return 'Translated text' }
async function updateCRM(userId: string, message: string, response: any, context: any): Promise<void> {}
async function handlePaymentIntent(message: string, userId: string): Promise<any> { return {} }
async function triggerEscalation(session: SuperSalSession, message: string, response: any): Promise<void> {}
async function saveSessionToDatabase(session: SuperSalSession): Promise<void> {}
function checkEscalationTriggers(message: string, session: SuperSalSession, response: any): boolean { return false }

export { SuperSalMessage, SuperSalSession }
